{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¬ Feature Importance & Model Comparison\n",
    "\n",
    "Este notebook foca na anÃ¡lise de feature importance e comparaÃ§Ã£o detalhada entre diferentes modelos.\n",
    "\n",
    "## Objetivos\n",
    "1. Analisar importÃ¢ncia das features\n",
    "2. Comparar performance de modelos individuais\n",
    "3. Otimizar threshold de sinais\n",
    "4. Visualizar curvas de decisÃ£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils.data_loader import DataLoader\n",
    "from features.technical_indicators import TechnicalIndicators\n",
    "from models.ml_models import TradingModel, EnsembleModel\n",
    "from backtesting.backtest_engine import BacktestEngine\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ… Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "loader = DataLoader()\n",
    "ticker = \"AAPL\"\n",
    "data = loader.download_stock_data(ticker, period=\"5y\")\n",
    "\n",
    "indicators = TechnicalIndicators()\n",
    "data_with_features = indicators.add_all_features(data)\n",
    "data_with_features['target'] = loader.create_target_variable(\n",
    "    data_with_features, horizon=5, threshold=0.01\n",
    ")\n",
    "\n",
    "data_clean = data_with_features.dropna()\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = loader.prepare_training_data(\n",
    "    data_clean, target_col='target', test_size=0.2, validation_size=0.1\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "print(f\"Features: {len(X_train.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "models = {}\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "models['RF'] = TradingModel(model_type='random_forest')\n",
    "models['RF'].fit(X_train, y_train, X_val, y_val, n_estimators=100, max_depth=10)\n",
    "\n",
    "print(\"Training XGBoost...\")\n",
    "models['XGB'] = TradingModel(model_type='xgboost')\n",
    "models['XGB'].fit(X_train, y_train, X_val, y_val, n_estimators=100, max_depth=6)\n",
    "\n",
    "print(\"Training LightGBM...\")\n",
    "models['LGBM'] = TradingModel(model_type='lightgbm')\n",
    "models['LGBM'].fit(X_train, y_train, X_val, y_val, n_estimators=100, max_depth=6)\n",
    "\n",
    "print(\"Training Logistic Regression...\")\n",
    "models['LR'] = TradingModel(model_type='logistic')\n",
    "models['LR'].fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"\\nâœ… All models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature importance across models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, model) in enumerate([('RF', models['RF']), ('XGB', models['XGB']), \n",
    "                                       ('LGBM', models['LGBM'])]):\n",
    "    importance = model.get_feature_importance(top_n=15)\n",
    "    \n",
    "    axes[idx].barh(range(len(importance)), importance['importance'])\n",
    "    axes[idx].set_yticks(range(len(importance)))\n",
    "    axes[idx].set_yticklabels(importance['feature'])\n",
    "    axes[idx].set_xlabel('Importance')\n",
    "    axes[idx].set_title(f'{name} - Top 15 Features', fontsize=12, fontweight='bold')\n",
    "    axes[idx].invert_yaxis()\n",
    "    axes[idx].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes[3])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common important features\n",
    "rf_top = set(models['RF'].get_feature_importance(top_n=10)['feature'].tolist())\n",
    "xgb_top = set(models['XGB'].get_feature_importance(top_n=10)['feature'].tolist())\n",
    "lgbm_top = set(models['LGBM'].get_feature_importance(top_n=10)['feature'].tolist())\n",
    "\n",
    "common_features = rf_top & xgb_top & lgbm_top\n",
    "\n",
    "print(\"\\nðŸ”¥ Top features common to all models:\")\n",
    "for feat in common_features:\n",
    "    print(f\"  - {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models on validation set\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_val, y_pred),\n",
    "        'Precision': precision_score(y_val, y_pred, average='weighted', zero_division=0),\n",
    "        'Recall': recall_score(y_val, y_pred, average='weighted', zero_division=0),\n",
    "        'F1': f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nðŸ“Š Model Comparison (Validation Set):\\n\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x - width*1.5, results_df['Accuracy'], width, label='Accuracy', alpha=0.8)\n",
    "ax.bar(x - width*0.5, results_df['Precision'], width, label='Precision', alpha=0.8)\n",
    "ax.bar(x + width*0.5, results_df['Recall'], width, label='Recall', alpha=0.8)\n",
    "ax.bar(x + width*1.5, results_df['F1'], width, label='F1', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results_df['Model'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, model) in enumerate(models.items()):\n",
    "    y_pred = model.predict(X_val)\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['Sell', 'Hold', 'Buy'],\n",
    "                yticklabels=['Sell', 'Hold', 'Buy'])\n",
    "    axes[idx].set_title(f'{name} - Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Probability Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction probabilities\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, model) in enumerate(models.items()):\n",
    "    proba = model.predict_proba(X_val)\n",
    "    \n",
    "    # Plot probability distribution for each class\n",
    "    axes[idx].hist(proba[:, 0], bins=30, alpha=0.5, label='Sell', color='red')\n",
    "    axes[idx].hist(proba[:, 1], bins=30, alpha=0.5, label='Hold', color='gray')\n",
    "    axes[idx].hist(proba[:, 2], bins=30, alpha=0.5, label='Buy', color='green')\n",
    "    \n",
    "    axes[idx].set_title(f'{name} - Probability Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Probability')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different thresholds\n",
    "ensemble = EnsembleModel([models['RF'], models['XGB'], models['LGBM']])\n",
    "ensemble.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "test_predictions = ensemble.predict_proba(X_test)\n",
    "test_data = data_clean.iloc[-len(X_test):].copy().reset_index(drop=True)\n",
    "\n",
    "backtest = BacktestEngine(initial_capital=100000, commission=0.001, slippage=0.0005)\n",
    "\n",
    "thresholds = np.arange(0.45, 0.70, 0.02)\n",
    "threshold_results = []\n",
    "\n",
    "print(\"Testing different thresholds...\\n\")\n",
    "for threshold in thresholds:\n",
    "    signals = backtest.generate_signals_from_predictions(test_predictions, threshold=threshold)\n",
    "    signals_series = pd.Series(signals).reset_index(drop=True)\n",
    "    \n",
    "    try:\n",
    "        results = backtest.run_backtest(test_data, signals_series, price_col='close')\n",
    "        threshold_results.append({\n",
    "            'Threshold': threshold,\n",
    "            'Total Return': results.total_return,\n",
    "            'Sharpe Ratio': results.sharpe_ratio,\n",
    "            'Max Drawdown': results.max_drawdown,\n",
    "            'Win Rate': results.win_rate,\n",
    "            'Total Trades': results.total_trades\n",
    "        })\n",
    "        print(f\"Threshold {threshold:.2f}: Return={results.total_return:.2%}, \"\n",
    "              f\"Sharpe={results.sharpe_ratio:.2f}, Trades={results.total_trades}\")\n",
    "    except:\n",
    "        print(f\"Threshold {threshold:.2f}: Failed (no valid trades)\")\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize threshold impact\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axes[0, 0].plot(threshold_df['Threshold'], threshold_df['Total Return'], marker='o')\n",
    "axes[0, 0].set_title('Total Return vs Threshold', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Threshold')\n",
    "axes[0, 0].set_ylabel('Total Return')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(threshold_df['Threshold'], threshold_df['Sharpe Ratio'], marker='o', color='orange')\n",
    "axes[0, 1].set_title('Sharpe Ratio vs Threshold', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Threshold')\n",
    "axes[0, 1].set_ylabel('Sharpe Ratio')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(threshold_df['Threshold'], threshold_df['Max Drawdown'], marker='o', color='red')\n",
    "axes[1, 0].set_title('Max Drawdown vs Threshold', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Threshold')\n",
    "axes[1, 0].set_ylabel('Max Drawdown')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(threshold_df['Threshold'], threshold_df['Total Trades'], marker='o', color='green')\n",
    "axes[1, 1].set_title('Total Trades vs Threshold', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Threshold')\n",
    "axes[1, 1].set_ylabel('Total Trades')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal threshold\n",
    "best_return_idx = threshold_df['Total Return'].idxmax()\n",
    "best_sharpe_idx = threshold_df['Sharpe Ratio'].idxmax()\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Optimal threshold for Total Return: {threshold_df.loc[best_return_idx, 'Threshold']:.2f}\")\n",
    "print(f\"   Return: {threshold_df.loc[best_return_idx, 'Total Return']:.2%}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Optimal threshold for Sharpe Ratio: {threshold_df.loc[best_sharpe_idx, 'Threshold']:.2f}\")\n",
    "print(f\"   Sharpe: {threshold_df.loc[best_sharpe_idx, 'Sharpe Ratio']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "1. **Feature Importance**: [Analyze which features are most predictive]\n",
    "2. **Model Performance**: [Compare which model performs best]\n",
    "3. **Optimal Threshold**: [Determine best threshold for signals]\n",
    "4. **Trade-offs**: [Balance between return, risk, and number of trades]\n",
    "\n",
    "### Next Steps:\n",
    "- Use optimal threshold for production\n",
    "- Focus on top features for faster training\n",
    "- Consider ensemble with only best models\n",
    "- Monitor performance over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
