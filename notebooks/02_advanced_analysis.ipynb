{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Advanced Feature Analysis & Model Interpretation\n",
    "\n",
    "This notebook demonstrates advanced techniques for:\n",
    "- Feature correlation analysis\n",
    "- SHAP value interpretation\n",
    "- Hyperparameter optimization with Optuna\n",
    "- Walk-forward validation\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have completed the basic tutorial first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('../src'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import optuna\n",
    "\n",
    "from utils.data_loader import DataLoader\n",
    "from features.technical_indicators import TechnicalIndicators\n",
    "from models.ml_models import TradingModel\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Suppress optuna logging\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "loader = DataLoader()\n",
    "data = loader.generate_synthetic_data(n_days=1000, random_state=42)\n",
    "\n",
    "indicators = TechnicalIndicators()\n",
    "data_with_features = indicators.add_all_features(data.copy())\n",
    "data_with_features['target'] = loader.create_target_variable(\n",
    "    data_with_features, horizon=5, threshold=0.01, binary=False\n",
    ")\n",
    "data_with_features = data_with_features.dropna()\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = loader.prepare_training_data(\n",
    "    data_with_features, target_col='target'\n",
    ")\n",
    "\n",
    "print(f\"âœ… Data prepared: {X_train.shape[0]} train, {X_val.shape[0]} val, {X_test.shape[0]} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Feature Correlation Analysis\n",
    "\n",
    "Understand relationships between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = X_train.corr()\n",
    "\n",
    "# Plot correlation heatmap for top features\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8},\n",
    "            annot=False, fmt='.2f')\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated features\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.9:\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"\\nâš ï¸ Highly Correlated Features (>0.9):\")\n",
    "    for feat1, feat2, corr in high_corr_pairs[:10]:\n",
    "        print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"\\nâœ… No highly correlated features found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Hyperparameter Optimization with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Objective function for Optuna optimization\"\"\"\n",
    "    \n",
    "    # Suggest hyperparameters\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = TradingModel(model_type='xgboost', **params)\n",
    "    model.fit(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    preds = model.predict(X_val)\n",
    "    score = accuracy_score(y_val, preds)\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Run optimization\n",
    "print(\"ðŸ” Starting hyperparameter optimization...\")\n",
    "study = optuna.create_study(direction='maximize', study_name='xgboost_optimization')\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nâœ… Optimization complete!\")\n",
    "print(f\"\\nðŸ† Best Parameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\nðŸ“Š Best Validation Accuracy: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot optimization history\n",
    "trial_values = [trial.value for trial in study.trials]\n",
    "axes[0].plot(trial_values, marker='o', linestyle='-', alpha=0.7)\n",
    "axes[0].axhline(y=study.best_value, color='r', linestyle='--', label=f'Best: {study.best_value:.4f}')\n",
    "axes[0].set_xlabel('Trial Number', fontsize=12)\n",
    "axes[0].set_ylabel('Validation Accuracy', fontsize=12)\n",
    "axes[0].set_title('Optimization Progress', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot parameter importance\n",
    "try:\n",
    "    importance = optuna.importance.get_param_importances(study)\n",
    "    params = list(importance.keys())\n",
    "    values = list(importance.values())\n",
    "    axes[1].barh(params, values)\n",
    "    axes[1].set_xlabel('Importance', fontsize=12)\n",
    "    axes[1].set_title('Parameter Importance', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3, axis='x')\n",
    "except:\n",
    "    axes[1].text(0.5, 0.5, 'Not enough trials\\nfor importance', \n",
    "                ha='center', va='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Train Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best parameters\n",
    "best_model = TradingModel(model_type='xgboost', **study.best_params)\n",
    "best_model.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_preds = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_preds)\n",
    "\n",
    "print(\"\\nðŸ“Š Test Set Performance:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ SHAP Value Analysis\n",
    "\n",
    "Understand which features contribute most to predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer\n",
    "print(\"ðŸ” Calculating SHAP values... (this may take a moment)\")\n",
    "explainer = shap.TreeExplainer(best_model.model)\n",
    "\n",
    "# Calculate SHAP values for test set (use subset for speed)\n",
    "shap_values = explainer.shap_values(X_test[:100])\n",
    "\n",
    "print(\"âœ… SHAP values calculated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test[:100], show=False)\n",
    "plt.title('SHAP Summary Plot - Feature Impact on Predictions', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from SHAP\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test[:100], plot_type=\"bar\", show=False)\n",
    "plt.title('SHAP Feature Importance', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze specific feature\n",
    "feature_to_analyze = X_test.columns[0]  # Change to analyze different features\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.dependence_plot(feature_to_analyze, shap_values, X_test[:100], show=False)\n",
    "plt.title(f'SHAP Dependence Plot - {feature_to_analyze}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Prediction Confidence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities\n",
    "test_probas = best_model.predict_proba(X_test)\n",
    "\n",
    "# Calculate confidence (max probability)\n",
    "confidence = np.max(test_probas, axis=1)\n",
    "\n",
    "# Analyze prediction confidence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confidence distribution\n",
    "axes[0].hist(confidence, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(x=confidence.mean(), color='r', linestyle='--', \n",
    "                label=f'Mean: {confidence.mean():.3f}')\n",
    "axes[0].set_xlabel('Confidence', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy by confidence level\n",
    "confidence_bins = [0, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "accuracy_by_conf = []\n",
    "samples_by_conf = []\n",
    "\n",
    "for i in range(len(confidence_bins)-1):\n",
    "    mask = (confidence >= confidence_bins[i]) & (confidence < confidence_bins[i+1])\n",
    "    if mask.sum() > 0:\n",
    "        acc = accuracy_score(y_test[mask], test_preds[mask])\n",
    "        accuracy_by_conf.append(acc)\n",
    "        samples_by_conf.append(mask.sum())\n",
    "    else:\n",
    "        accuracy_by_conf.append(0)\n",
    "        samples_by_conf.append(0)\n",
    "\n",
    "bin_labels = [f'{confidence_bins[i]:.1f}-{confidence_bins[i+1]:.1f}' \n",
    "              for i in range(len(confidence_bins)-1)]\n",
    "\n",
    "x = np.arange(len(bin_labels))\n",
    "width = 0.35\n",
    "\n",
    "ax2 = axes[1]\n",
    "bars = ax2.bar(x, accuracy_by_conf, width, label='Accuracy', alpha=0.7)\n",
    "ax2.set_xlabel('Confidence Range', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.set_title('Accuracy by Confidence Level', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(bin_labels, rotation=45)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add sample counts on top of bars\n",
    "for i, (bar, count) in enumerate(zip(bars, samples_by_conf)):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'n={count}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Confidence Statistics:\")\n",
    "print(f\"Mean Confidence: {confidence.mean():.4f}\")\n",
    "print(f\"Std Confidence: {confidence.std():.4f}\")\n",
    "print(f\"High Confidence (>0.7): {(confidence > 0.7).sum()} samples ({(confidence > 0.7).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "From this analysis, you've learned:\n",
    "\n",
    "1. **Feature Correlations**: Which features are redundant\n",
    "2. **Hyperparameter Optimization**: How to find optimal model parameters\n",
    "3. **SHAP Values**: Which features drive predictions\n",
    "4. **Prediction Confidence**: How confident the model is in its predictions\n",
    "\n",
    "## ðŸ“š Next Steps\n",
    "\n",
    "- Remove highly correlated features to reduce overfitting\n",
    "- Focus on the most important features from SHAP analysis\n",
    "- Use confidence levels to filter trading signals\n",
    "- Experiment with different model architectures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
